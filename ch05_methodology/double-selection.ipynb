{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "architectural-defendant",
   "metadata": {},
   "source": [
    "# Section 5.4: The regularization bias, simulations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e15b07",
   "metadata": {},
   "source": [
    "References:\n",
    "- Beck, Teboule (2088) \"A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems\"\n",
    "- Belloni, A., Chernozhukov, V., Hansen, C. (2011) \"Inference on Treatment Effects After Selection Amongst High-Dimensional Controls\", https://arxiv.org/abs/1201.0224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ml4econometrics.ols import OLS\n",
    "from ml4econometrics.penalized_regressions import Lasso\n",
    "\n",
    "np.random.seed(99999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-drink",
   "metadata": {},
   "source": [
    "# DGP and estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed80d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_double_selection(X, d, y, s_hat):\n",
    "    ols = OLS()\n",
    "    if len(s_hat) > 0:\n",
    "        X_ = np.c_[d, X[:, s_hat]]\n",
    "    else:\n",
    "        X_ = d.reshape(-1, 1)\n",
    "    \n",
    "    ols.fit(X_, y)\n",
    "    ols.compute_std(X=X_, y=y)\n",
    "    return ols.coef_[0], ols.std[1]\n",
    "\n",
    "def double_selection_estimator(X, d, y, alpha):\n",
    "    n = len(X)\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    ## A. Selection on treatment\n",
    "    lasso.fit(X, d)\n",
    "    s_d = np.nonzero(lasso.coef_)[0]\n",
    "\n",
    "    ## B. Selection on outcome\n",
    "    lasso.fit(X, y)\n",
    "    s_y = np.nonzero(lasso.coef_)[0]\n",
    "\n",
    "    ## C. Compute double selection estimator\n",
    "    s_hat = np.union1d(s_y, s_d)\n",
    "    if len(s_hat) > 0:\n",
    "        X_ = np.c_[d, X[:, s_hat]]\n",
    "    else:\n",
    "        X_ = d.reshape(-1, 1)\n",
    "\n",
    "    ols = OLS()\n",
    "    ols.fit(X_, y)\n",
    "    tau_hat = ols.coef_[0]\n",
    "    eps_y = y - ols.predict(X_)\n",
    "\n",
    "    if len(s_hat) > 0:\n",
    "        ols.fit(X[:, s_hat], d)\n",
    "        eps_d = d - ols.predict(X[:, s_hat])\n",
    "    else:\n",
    "        eps_d = d - d.mean()\n",
    "\n",
    "    tau_std = np.sum((eps_d * eps_y) ** 2) / (n - len(s_hat) - 1)\n",
    "    tau_std /= (np.sum(eps_d ** 2) / n) ** 2\n",
    "    tau_std = np.sqrt(tau_std) / np.sqrt(n)\n",
    "\n",
    "    return tau_hat, tau_std, s_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DGP():\n",
    "    \"\"\"\n",
    "    Generates synthetic data for illustrating the regularization bias.\n",
    "\n",
    "    Args:\n",
    "        n (int): Number of samples.\n",
    "        p (int): Number of variables.\n",
    "        r_y (float): R-squared for the outcome equation.\n",
    "        r_d (float): R-squared for the treatment variable.\n",
    "        intercept (bool): Whether to include an intercept term.\n",
    "        rho (float): Covariate correlation parameter.\n",
    "        tau (float): Treatment effect.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the generated data:\n",
    "            X (numpy.ndarray): Covariate matrix.\n",
    "            y (numpy.ndarray): Outcome variable.\n",
    "            d (numpy.ndarray): Treatment variable.\n",
    "    \"\"\"\n",
    "    n: int = 200\n",
    "    p: int = 300\n",
    "    r_y: float = .1\n",
    "    r_d: float = .8\n",
    "    intercept: bool = True\n",
    "    rho: float = .5\n",
    "    tau: float = .5\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        p = self.p\n",
    "        # Covariate variance matrix\n",
    "        self.sigma = np.array([[self.rho ** np.abs(k - j) for k in range(p)] for j in range(p)])\n",
    "\n",
    "        # Treatment variable coefficient\n",
    "        beta = np.zeros(p)\n",
    "        for j in range(p // 2):\n",
    "            beta[j] = (-1) ** (j+1) / (j+1) ** 2\n",
    "\n",
    "        # Outcome equation coefficients\n",
    "        gamma = np.copy(beta)\n",
    "        for j in range(p // 2 + 1, p+1):\n",
    "            gamma[j-1] = (-1) ** (j + 1) / (p - j + 1) ** 2\n",
    "\n",
    "        # Adjustment to match R-squared\n",
    "        gamma *= np.sqrt(self.r_d / (1 - self.r_d) / (gamma.T @ self.sigma @ gamma))\n",
    "        beta *= np.sqrt(self.r_y / (1 - self.r_y) / (beta.T @ self.sigma @ beta))\n",
    "        \n",
    "        self.gamma, self.beta = gamma, beta\n",
    "        \n",
    "        # All even-indexed covariates are dummies\n",
    "        self.even = np.arange(1, p + 1) % 2 == 0\n",
    "    \n",
    "    def generate(self):\n",
    "        # Generate covariates\n",
    "        X = multivariate_normal(mean=np.zeros(self.p), cov=self.sigma).rvs(size=self.n)\n",
    "        X[:, self.even] = np.where(X[:, self.even] > 0, 1, 0)\n",
    "\n",
    "        # Treatment\n",
    "        d = 1 * np.array(np.random.rand(self.n) < norm.cdf(X @ self.gamma))\n",
    "\n",
    "        # Outcome\n",
    "        y = self.tau * d + X @ self.beta + np.random.randn(self.n)\n",
    "\n",
    "        if self.intercept:\n",
    "            X = np.c_[np.ones(self.n), X]\n",
    "\n",
    "        return X, y, d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-bachelor",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgp = DGP(n=200, p=300, tau=.5, intercept=False, r_y=.1, r_d=.8)\n",
    "n_sim = 100   # nb simulations\n",
    "n_folds = 5   # nb folds\n",
    "\n",
    "# Generate random split\n",
    "cvgroup = np.digitize(np.random.rand(dgp.n), np.linspace(0, 1, n_folds + 1))\n",
    "\n",
    "# Penalty level\n",
    "g = .1 / np.log(max(dgp.n, dgp.p))\n",
    "c = 1.1\n",
    "alpha = 2 * c * norm.ppf(1 - .5 * g / dgp.p) / np.sqrt(dgp.n) # (theoretical) Lasso penalty level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-favorite",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_naive = Lasso(alpha=alpha, nopen=[0])\n",
    "ols = OLS()\n",
    "\n",
    "estimate, std = [], []\n",
    "\n",
    "for b in tqdm(range(n_sim)):\n",
    "    X, y, d  = dgp.generate()\n",
    "    X_ = np.c_[d, X]\n",
    "\n",
    "    # Method 1: Naive selection\n",
    "    lasso_naive.fit(X_, y)\n",
    "    s_hat = np.nonzero(lasso_naive.coef_[1:])[0]\n",
    "\n",
    "    tau_naive, std_naive = compute_double_selection(X, d, y, s_hat)\n",
    "\n",
    "    # Method 2: Double-Selection, no sample-splitting\n",
    "    tau_ds, std_ds, _ = double_selection_estimator(X, d, y, alpha)\n",
    "\n",
    "    # Method 3: with sample splitting\n",
    "    tau_k, sigma_k_num, sigma_k_denom = [], [], []\n",
    "    for k in range(1, n_folds + 1):\n",
    "        Ik = (cvgroup == k)\n",
    "        NIk = (cvgroup != k)\n",
    "\n",
    "        g_cf = .1 / np.log(max(NIk.sum(), dgp.p))\n",
    "        alpha_cf = 1.1 * norm.ppf(1 - .5 * g_cf / dgp.p) / np.sqrt(NIk.sum())\n",
    "\n",
    "        _, _, s_hat = double_selection_estimator(X[NIk], d[NIk], y[NIk], alpha_cf)\n",
    "\n",
    "        if len(s_hat) > 0:\n",
    "            ols.fit(X[np.ix_(NIk, s_hat)], y[NIk])\n",
    "            eps_y = y[Ik] - ols.predict(X[np.ix_(Ik, s_hat)])\n",
    "\n",
    "            ols.fit(X[np.ix_(NIk, s_hat)], d[NIk])\n",
    "            eps_d = d[Ik] - ols.predict(X[np.ix_(Ik, s_hat)])\n",
    "        else:\n",
    "            eps_y = y[Ik] - y[NIk].mean()\n",
    "            eps_d = d[Ik] - d[NIk].mean()\n",
    "            \n",
    "        ols.fit(eps_d, eps_y)\n",
    "        tau_k.append(ols.coef_[0])\n",
    "\n",
    "        sigma_k_num.append(np.sum((eps_d*eps_y) ** 2))\n",
    "        sigma_k_denom.append(np.sum(eps_d ** 2))\n",
    "\n",
    "    sigma_k_num = np.array(sigma_k_num).sum() / len(X)\n",
    "    sigma_k_denom = np.array(sigma_k_denom).sum() / len(X)\n",
    "\n",
    "    # Save results\n",
    "    estimate.append({\n",
    "        'Naive post-selection': tau_naive,\n",
    "        'Double selection': tau_ds,\n",
    "        'Double selection w/ cross-fitting': np.array(tau_k).mean()\n",
    "    })\n",
    "    std.append({\n",
    "        'Naive post-selection': std_naive,\n",
    "        'Double selection': std_ds,\n",
    "        'Double selection w/ cross-fitting': np.sqrt(sigma_k_num / sigma_k_denom ** 2) / np.sqrt(len(X))\n",
    "    })\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d134f2",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate = pd.DataFrame(estimate)\n",
    "std = pd.DataFrame(std)\n",
    "\n",
    "table = {\n",
    "    'Bias': estimate.mean() - dgp.tau,\n",
    "    'RMSE': (estimate - dgp.tau).pow(2).mean().pow(.5),\n",
    "    'Coverage rate': (np.abs(estimate - dgp.tau) < norm.ppf(0.975) * std).mean() # from (a+b)(a-b) < 0 iff abs(a) < b (and b > 0 always here), i.e. bounds of CI surround 0.\n",
    "}\n",
    "print(pd.DataFrame(table).T.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(table).T.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771e8a2",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10cd1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_ = (estimate - dgp.tau) / std\n",
    "fig, axs = plt.subplots(1, len(estimate_.columns), figsize=(20, 8), sharex=True, sharey=True, dpi=800)\n",
    "axs = axs.ravel()\n",
    "\n",
    "x = np.linspace(-1.1 * np.abs(estimate_).max().max(), 1.1 * np.abs(estimate_).max().max(), 100)\n",
    "\n",
    "for ax, (title, data) in zip(axs, estimate_.items()):\n",
    "    # Plot histogram with light grey color\n",
    "    ax.hist(data, bins=int(n_sim ** (2 / 3)), color='lightgrey', density=True)\n",
    "    ax.plot(x, norm.pdf(x, 0, 1), color='black', linestyle='dashed', linewidth=2)\n",
    "    \n",
    "    # Set plot title and labels\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(r'$\\sqrt{n}(\\widehat \\tau - \\tau_0) / \\widehat \\sigma$')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Fig_5_1.jpg\", bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
