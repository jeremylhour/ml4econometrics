{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "architectural-defendant",
   "metadata": {},
   "source": [
    "# Section 5.4: The regularization bias, simulations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e15b07",
   "metadata": {},
   "source": [
    "References:\n",
    "- Beck, Teboule (2088) \"A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems\"\n",
    "- Belloni, A., Chernozhukov, V., Hansen, C. (2011) \"Inference on Treatment Effects After Selection Amongst High-Dimensional Controls\", https://arxiv.org/abs/1201.0224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "np.random.seed(99999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-drink",
   "metadata": {},
   "source": [
    "# DGP and estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed80d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------\n",
    "# OLS\n",
    "#------------------------------------------------------\n",
    "@dataclass\n",
    "class OLS(BaseEstimator):\n",
    "    \"\"\"\n",
    "    OLS:\n",
    "        Ordinary Least Squares,\n",
    "        but outputs variance, etc.\n",
    "    \"\"\"\n",
    "    fit_intercept: bool = True\n",
    "    robust_variance: bool = True\n",
    "    rcond: float = 1e-15\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        estimate:\n",
    "            Compute the OLS coefficient\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Dimension (n, p).\n",
    "            y (np.array): Dimension (n,).\n",
    "        \"\"\"\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(len(X)), X]\n",
    "\n",
    "        beta, _, rank_, singular_ = np.linalg.lstsq(X, y, rcond=None)\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = beta[0]\n",
    "            self.coef_ = beta[1:]\n",
    "        else:\n",
    "            self.intercept_ = 0.\n",
    "            self.coef_ = beta\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'coef_')\n",
    "        check_is_fitted(self, 'intercept_')\n",
    "        X = check_array(X)\n",
    "        return self.intercept_ + X @ self.coef_\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        return r2_score(y, y_hat)\n",
    "    \n",
    "    def compute_std(self, X, y, robust=True):\n",
    "        \"\"\"\n",
    "        compute_std:\n",
    "        \n",
    "        Args:\n",
    "            X (np.array): Dimension (n, p).\n",
    "            y (np.array): Dimension (n,).\n",
    "            robust (bool): If True, uses White sandwich variance estimator. \n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'coef_')\n",
    "        check_is_fitted(self, 'intercept_')\n",
    "        \n",
    "        epsilon = y - self.predict(X)\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            X = np.c_[np.ones(len(X)), X]\n",
    "\n",
    "        XtX = X.T @ X\n",
    "        inv_XtX = np.linalg.pinv(XtX, rcond=self.rcond, hermitian=True)\n",
    "\n",
    "        if robust:\n",
    "            A = (inv_XtX @ X.T) * epsilon\n",
    "            self.variance_ =  len(X) * A @ A.T\n",
    "        else:\n",
    "            self.variance_ =  len(X) * np.sum(epsilon ** 2) * inv_XtX / (len(X) - 1)\n",
    "\n",
    "        self.std = np.sqrt(np.diag(self.variance_) / len(X))\n",
    "\n",
    "    def compute_student_statistics(self, X, y):\n",
    "        check_is_fitted(self, 'coef_')\n",
    "\n",
    "        if not hasattr(self, \"variance_\"):\n",
    "            self.compute_std(X=X, y=y, robust=self.robust_variance)\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            self.t_stat = self.coef_ / self.std[1:]\n",
    "        else:\n",
    "            self.t_stat = self.coef_ / self.std\n",
    "        self.p_value = 1 - norm.cdf(np.abs(self.t_stat))\n",
    "\n",
    "#------------------------------------------------------\n",
    "# LASSO\n",
    "#------------------------------------------------------\n",
    "@dataclass\n",
    "class Lasso(BaseEstimator):\n",
    "    \"\"\"\n",
    "    _PenalizedRegression:\n",
    "        Penalized regression parent class.\n",
    "\n",
    "    Args:\n",
    "        alpha (float): Overall penalty strength.\n",
    "        gamma (float): Weight of the Lasso penalty vs. Group-Lasso penalty.\n",
    "            1 = lasso, 0 = group-lasso.\n",
    "        groups (list of lists of integers): List of feature indices for each group.\n",
    "        nopen (list of int): Index of variables that should not be penalized.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "        tol (float): Tolerance for convergence.\n",
    "        fit_intercept (bool): If True, fits an intercept.\n",
    "        verbose (bool): If True, displays progress.\n",
    "\n",
    "    Attributes\n",
    "    coef_ : array, shape (n_features,)\n",
    "        Estimated coefficients.\n",
    "    \"\"\"\n",
    "    alpha: float = 1.\n",
    "    nopen: List[int] = field(default_factory=list)\n",
    "    fit_intercept: bool = True\n",
    "    verbose: bool = False\n",
    "    max_iter: int = 5_000\n",
    "    tol: float = 1e-6\n",
    "    static_args: dict = field(default_factory=dict)\n",
    "\n",
    "    def loss_gradient(self, w, b, y, X):\n",
    "        return - 2 * (y - b - X @ w) @ X / len(X)\n",
    "    \n",
    "    def intercept_loss_gradient(self, w, b, y, X):\n",
    "        return - 2 * (y - b - X @ w).mean()\n",
    "        \n",
    "    def prox(self, x, alpha, nopen):\n",
    "        y = np.sign(x) * np.maximum(0, np.abs(x) - alpha)\n",
    "        if nopen is not None:\n",
    "            y[nopen] = x[nopen]\n",
    "        return y\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        n_, n_features = X.shape\n",
    "        \n",
    "        # Initialize constants and variables\n",
    "        _, w, _ = np.linalg.svd(X)\n",
    "        eta = n_ / (2 * w[0] ** 2)\n",
    "        t, t_old = 1., 1.\n",
    "        coef, z = np.zeros(n_features), np.zeros(n_features)\n",
    "        b = 0.\n",
    "\n",
    "        if self.verbose:\n",
    "            loop = tqdm(range(self.max_iter))\n",
    "        else:\n",
    "            loop = range(self.max_iter)\n",
    "\n",
    "        for _ in loop:\n",
    "            # Save parameters\n",
    "            coef_old = coef.copy() \n",
    "            t_old = t\n",
    "            t = (1 + np.sqrt(1 + 4 * t_old ** 2)) / 2\n",
    "            delta = (1 - t_old) / t\n",
    "\n",
    "            # Compute gradient and update coefficients\n",
    "            grad = self.loss_gradient(z, b, y, X)\n",
    "            coef = self.prox(\n",
    "                x=z - eta * grad,\n",
    "                alpha=eta * self.alpha,\n",
    "                nopen=self.nopen\n",
    "                )\n",
    "            if self.fit_intercept:\n",
    "                b  -= eta * self.intercept_loss_gradient(w=coef, b=b, y=y, X=X)\n",
    "            z = (1 - delta) * coef + delta * coef_old\n",
    "\n",
    "            # Check convergence\n",
    "            if np.linalg.norm(coef - coef_old) / np.maximum(1e-9, np.linalg.norm(coef)) < self.tol:\n",
    "                break\n",
    "        if (_ == self.max_iter - 1):\n",
    "            print(\"Max. number of iterations reached.\")\n",
    "        self.coef_ = coef\n",
    "        self.intercept_ = b\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'coef_')\n",
    "        check_is_fitted(self, 'intercept_')\n",
    "        X = check_array(X)\n",
    "        return self.intercept_ + X @ self.coef_\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        return r2_score(y, y_hat)\n",
    "    \n",
    "def compute_double_selection(X, d, y, s_hat):\n",
    "    ols = OLS()\n",
    "    if len(s_hat) > 0:\n",
    "        X_ = np.c_[d, X[:, s_hat]]\n",
    "    else:\n",
    "        X_ = d.reshape(-1, 1)\n",
    "    \n",
    "    ols.fit(X_, y)\n",
    "    ols.compute_std(X=X_, y=y)\n",
    "    return ols.coef_[0], ols.std[1]\n",
    "\n",
    "def double_selection_estimator(X, d, y, alpha):\n",
    "    n = len(X)\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    ## A. Selection on treatment\n",
    "    lasso.fit(X, d)\n",
    "    s_d = np.nonzero(lasso.coef_)[0]\n",
    "\n",
    "    ## B. Selection on outcome\n",
    "    lasso.fit(X, y)\n",
    "    s_y = np.nonzero(lasso.coef_)[0]\n",
    "\n",
    "    ## C. Compute double selection estimator\n",
    "    s_hat = np.union1d(s_y, s_d)\n",
    "    if len(s_hat) > 0:\n",
    "        X_ = np.c_[d, X[:, s_hat]]\n",
    "    else:\n",
    "        X_ = d.reshape(-1, 1)\n",
    "\n",
    "    ols = OLS()\n",
    "    ols.fit(X_, y)\n",
    "    tau_hat = ols.coef_[0]\n",
    "    eps_y = y - ols.predict(X_)\n",
    "\n",
    "    if len(s_hat) > 0:\n",
    "        ols.fit(X[:, s_hat], d)\n",
    "        eps_d = d - ols.predict(X[:, s_hat])\n",
    "    else:\n",
    "        eps_d = d - d.mean()\n",
    "\n",
    "    tau_std = np.sum((eps_d * eps_y) ** 2) / (n - len(s_hat) - 1)\n",
    "    tau_std /= (np.sum(eps_d ** 2) / n) ** 2\n",
    "    tau_std = np.sqrt(tau_std) / np.sqrt(n)\n",
    "\n",
    "    return tau_hat, tau_std, s_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DGP():\n",
    "    \"\"\"\n",
    "    Generates synthetic data for illustrating the regularization bias.\n",
    "\n",
    "    Args:\n",
    "        n (int): Number of samples.\n",
    "        p (int): Number of variables.\n",
    "        r_y (float): R-squared for the outcome equation.\n",
    "        r_d (float): R-squared for the treatment variable.\n",
    "        intercept (bool): Whether to include an intercept term.\n",
    "        rho (float): Covariate correlation parameter.\n",
    "        tau (float): Treatment effect.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the generated data:\n",
    "            X (numpy.ndarray): Covariate matrix.\n",
    "            y (numpy.ndarray): Outcome variable.\n",
    "            d (numpy.ndarray): Treatment variable.\n",
    "    \"\"\"\n",
    "    n: int = 200\n",
    "    p: int = 300\n",
    "    r_y: float = .1\n",
    "    r_d: float = .8\n",
    "    intercept: bool = True\n",
    "    rho: float = .5\n",
    "    tau: float = .5\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        p = self.p\n",
    "        # Covariate variance matrix\n",
    "        self.sigma = np.array([[self.rho ** np.abs(k - j) for k in range(p)] for j in range(p)])\n",
    "\n",
    "        # Treatment variable coefficient\n",
    "        beta = np.zeros(p)\n",
    "        for j in range(p // 2):\n",
    "            beta[j] = (-1) ** (j+1) / (j+1) ** 2\n",
    "\n",
    "        # Outcome equation coefficients\n",
    "        gamma = np.copy(beta)\n",
    "        for j in range(p // 2 + 1, p+1):\n",
    "            gamma[j-1] = (-1) ** (j + 1) / (p - j + 1) ** 2\n",
    "\n",
    "        # Adjustment to match R-squared\n",
    "        gamma *= np.sqrt(self.r_d / (1 - self.r_d) / (gamma.T @ self.sigma @ gamma))\n",
    "        beta *= np.sqrt(self.r_y / (1 - self.r_y) / (beta.T @ self.sigma @ beta))\n",
    "        \n",
    "        self.gamma, self.beta = gamma, beta\n",
    "        \n",
    "        # All even-indexed covariates are dummies\n",
    "        self.even = np.arange(1, p + 1) % 2 == 0\n",
    "    \n",
    "    def generate(self):\n",
    "        # Generate covariates\n",
    "        X = multivariate_normal(mean=np.zeros(self.p), cov=self.sigma).rvs(size=self.n)\n",
    "        X[:, self.even] = np.where(X[:, self.even] > 0, 1, 0)\n",
    "\n",
    "        # Treatment\n",
    "        d = 1 * np.array(np.random.rand(self.n) < norm.cdf(X @ self.gamma))\n",
    "\n",
    "        # Outcome\n",
    "        y = self.tau * d + X @ self.beta + np.random.randn(self.n)\n",
    "\n",
    "        if self.intercept:\n",
    "            X = np.c_[np.ones(self.n), X]\n",
    "\n",
    "        return X, y, d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-bachelor",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgp = DGP(n=200, p=300, tau=.5, intercept=False, r_y=.1, r_d=.8)\n",
    "n_sim = 100   # nb simulations\n",
    "n_folds = 5   # nb folds\n",
    "\n",
    "# Generate random split\n",
    "cvgroup = np.digitize(np.random.rand(dgp.n), np.linspace(0, 1, n_folds + 1))\n",
    "\n",
    "# Penalty level\n",
    "g = .1 / np.log(max(dgp.n, dgp.p))\n",
    "c = 1.1\n",
    "alpha = 2 * c * norm.ppf(1 - .5 * g / dgp.p) / np.sqrt(dgp.n) # (theoretical) Lasso penalty level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-favorite",
   "metadata": {},
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_naive = Lasso(alpha=alpha, nopen=[0])\n",
    "ols = OLS()\n",
    "\n",
    "estimate, std = [], []\n",
    "\n",
    "for b in tqdm(range(n_sim)):\n",
    "    X, y, d  = dgp.generate()\n",
    "    X_ = np.c_[d, X]\n",
    "\n",
    "    # Method 1: Naive selection\n",
    "    lasso_naive.fit(X_, y)\n",
    "    s_hat = np.nonzero(lasso_naive.coef_[1:])[0]\n",
    "\n",
    "    tau_naive, std_naive = compute_double_selection(X, d, y, s_hat)\n",
    "\n",
    "    # Method 2: Double-Selection, no sample-splitting\n",
    "    tau_ds, std_ds, _ = double_selection_estimator(X, d, y, alpha)\n",
    "\n",
    "    # Method 3: with sample splitting\n",
    "    tau_k, sigma_k_num, sigma_k_denom = [], [], []\n",
    "    for k in range(1, n_folds + 1):\n",
    "        Ik = (cvgroup == k)\n",
    "        NIk = (cvgroup != k)\n",
    "\n",
    "        g_cf = .1 / np.log(max(NIk.sum(), dgp.p))\n",
    "        alpha_cf = 1.1 * norm.ppf(1 - .5 * g_cf / dgp.p) / np.sqrt(NIk.sum())\n",
    "\n",
    "        _, _, s_hat = double_selection_estimator(X[NIk], d[NIk], y[NIk], alpha_cf)\n",
    "\n",
    "        if len(s_hat) > 0:\n",
    "            ols.fit(X[np.ix_(NIk, s_hat)], y[NIk])\n",
    "            eps_y = y[Ik] - ols.predict(X[np.ix_(Ik, s_hat)])\n",
    "\n",
    "            ols.fit(X[np.ix_(NIk, s_hat)], d[NIk])\n",
    "            eps_d = d[Ik] - ols.predict(X[np.ix_(Ik, s_hat)])\n",
    "        else:\n",
    "            eps_y = y[Ik] - y[NIk].mean()\n",
    "            eps_d = d[Ik] - d[NIk].mean()\n",
    "            \n",
    "        ols.fit(eps_d, eps_y)\n",
    "        tau_k.append(ols.coef_[0])\n",
    "\n",
    "        sigma_k_num.append(np.sum((eps_d*eps_y) ** 2))\n",
    "        sigma_k_denom.append(np.sum(eps_d ** 2))\n",
    "\n",
    "    sigma_k_num = np.array(sigma_k_num).sum() / len(X)\n",
    "    sigma_k_denom = np.array(sigma_k_denom).sum() / len(X)\n",
    "\n",
    "    # Save results\n",
    "    estimate.append({\n",
    "        'Naive post-selection': tau_naive,\n",
    "        'Double selection': tau_ds,\n",
    "        'Double selection w/ cross-fitting': np.array(tau_k).mean()\n",
    "    })\n",
    "    std.append({\n",
    "        'Naive post-selection': std_naive,\n",
    "        'Double selection': std_ds,\n",
    "        'Double selection w/ cross-fitting': np.sqrt(sigma_k_num / sigma_k_denom ** 2) / np.sqrt(len(X))\n",
    "    })\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d134f2",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate = pd.DataFrame(estimate)\n",
    "std = pd.DataFrame(std)\n",
    "\n",
    "table = {\n",
    "    'Bias': estimate.mean() - dgp.tau,\n",
    "    'RMSE': (estimate - dgp.tau).pow(2).mean().pow(.5),\n",
    "    'Coverage rate': (np.abs(estimate - dgp.tau) < norm.ppf(0.975) * std).mean() # from (a+b)(a-b) < 0 iff abs(a) < b (and b > 0 always here), i.e. bounds of CI surround 0.\n",
    "}\n",
    "print(pd.DataFrame(table).T.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(table).T.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771e8a2",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10cd1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_ = (estimate - dgp.tau) / std\n",
    "fig, axs = plt.subplots(1, len(estimate_.columns), figsize=(20, 8), sharex=True, sharey=True, dpi=800)\n",
    "axs = axs.ravel()\n",
    "\n",
    "x = np.linspace(-1.1 * np.abs(estimate_).max().max(), 1.1 * np.abs(estimate_).max().max(), 100)\n",
    "\n",
    "for ax, (title, data) in zip(axs, estimate_.items()):\n",
    "    # Plot histogram with light grey color\n",
    "    ax.hist(data, bins=int(n_sim ** (2 / 3)), color='lightgrey', density=True)\n",
    "    ax.plot(x, norm.pdf(x, 0, 1), color='black', linestyle='dashed', linewidth=2)\n",
    "    \n",
    "    # Set plot title and labels\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(r'$\\sqrt{n}(\\widehat \\tau - \\tau_0) / \\widehat \\sigma$')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Fig_5_1.jpg\", bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
