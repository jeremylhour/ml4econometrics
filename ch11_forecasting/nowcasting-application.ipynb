{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "general-playlist",
   "metadata": {},
   "source": [
    "# Nowcasting in High-dimension: an application\n",
    "\n",
    "Chapter 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "athletic-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from src.stats_tools import SparseGroupLasso, SVD\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-chicago",
   "metadata": {},
   "source": [
    "# 0. Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-combine",
   "metadata": {},
   "source": [
    "## A. Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "static-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lags(x, l_min: int = 0, l_max: int = 1):\n",
    "    \"\"\"\n",
    "    create_lags:\n",
    "        Returns everything with the lags.\n",
    "    \n",
    "    Args:\n",
    "        x (pd.DataFrame): The dataframe to consider, indexed by the time.\n",
    "        l_min (int): Minimum of lags to have.\n",
    "        l_max (int): Maximum of lags to add.\n",
    "        \n",
    "    Return:\n",
    "        A concatenated dataset with all the lags for all series.\n",
    "    \"\"\"\n",
    "    lags = pd.concat({\n",
    "        l: x.shift(l) for l in range(l_min, l_max+1)\n",
    "    }, axis=1)\n",
    "    return lags\n",
    "\n",
    "def ts_standardize(x, halflife: int = 12, ignore_na: bool = True):\n",
    "    \"\"\"\n",
    "    ts_standardize:\n",
    "\n",
    "    Args:\n",
    "        x (pd.DataFrame):\n",
    "        halflife (int):\n",
    "        ignore_na (bool):\n",
    "    \"\"\"\n",
    "    return (\n",
    "        x\n",
    "        .sub(x.ewm(halflife=halflife, ignore_na=ignore_na).mean(), axis=1)\n",
    "        .div(x.ewm(halflife=halflife, ignore_na=ignore_na).std(), axis=1)\n",
    "    )\n",
    "\n",
    "def legendre_polynomials(degree, a=0, b=1, jmax=None, X=None):\n",
    "    \"\"\"\n",
    "    legendre_polynomials:\n",
    "        Legendre polynomials shifted to [a,b]\n",
    "        Source: https://github.com/jstriaukas/midasmlpy/blob/master/midasmlpy/src/midas_polynomials.py\n",
    "\n",
    "    Args:\n",
    "        degree (int): Degree of the polynomial.\n",
    "        a (float): Lower shift value (i.e., default - 0).\n",
    "        b (float): Upper shift value (i.e., default - 1).\n",
    "        jmax (int): Number of high-frequency lags.\n",
    "        X (ndarray): Optional evaluation grid vector.\n",
    "    \n",
    "    Return\n",
    "        Psi (ndarray): Weight matrix with Legendre functions up to ``degree``.\n",
    "    \"\"\"\n",
    "    if jmax is not None:\n",
    "        X = np.linspace(start=0, stop=1, num=jmax)\n",
    "\n",
    "    if X is None:\n",
    "        raise ValueError(\"X is not provided. Either set X or set jmax.\")\n",
    "\n",
    "    n = len(X)\n",
    "    P = np.ones([n, degree+2])\n",
    "    Psi = np.ones([n, degree+1]) / np.sqrt(b-a)\n",
    "    P[:, 1] = 2 * X / (b-a) - ((b+a) / (b-a))\n",
    "    if degree > 0:\n",
    "        for i in range(1, degree+1, 1):\n",
    "            P[:, i+1] = ((2*i+1)/(i+1)) * P[:, 1] * P[:, i] - i/(i+1) * P[:, i-1]\n",
    "            Psi[:, i] = np.sqrt((2*i + 1) / (b-a)) * P[:, i]\n",
    "    return Psi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-chapter",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-barrier",
   "metadata": {},
   "source": [
    "## A. Topic attention data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rough-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"http://www.structureofnews.com/data/download/Monthly_Topic_Attention_Theta.csv\"\n",
    "SELECTED_COLS = [\n",
    "    i-2 for i in [43,37,20,2,42,15,35,6,7,16,38,181,71,73,11,17,40,77,119,142,145,147,167,170,176]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "legendary-exclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402, 25)\n",
      "            Announce plan  Revised estimate  Record high  Natural disasters  \\\n",
      "date                                                                          \n",
      "1984-01-01       0.467674          0.688088     0.480592           0.339301   \n",
      "1984-02-01       0.424175          0.656877     0.417940           0.463997   \n",
      "1984-03-01       0.408318          0.658685     0.444813           0.529381   \n",
      "1984-04-01       0.392498          0.623401     0.448515           0.418133   \n",
      "1984-05-01       0.412422          0.558719     0.420943           0.417861   \n",
      "\n",
      "            Problems  Middle east  US defense   Profits       M&A  \\\n",
      "date                                                                \n",
      "1984-01-01  0.538521     0.746824    0.810809  0.962193  0.458591   \n",
      "1984-02-01  0.555107     1.085275    0.913313  0.895614  0.520514   \n",
      "1984-03-01  0.510741     0.849796    0.952218  0.552926  0.611201   \n",
      "1984-04-01  0.510228     0.768855    0.814239  1.093373  0.457250   \n",
      "1984-05-01  0.545848     0.627426    0.827564  0.562163  0.470433   \n",
      "\n",
      "            Savings & loans  ...      IPOs  Credit ratings  Earnings losses  \\\n",
      "date                         ...                                              \n",
      "1984-01-01         0.811011  ...  0.345154        0.594836         0.968248   \n",
      "1984-02-01         0.806113  ...  0.340305        0.540224         0.946901   \n",
      "1984-03-01         1.067591  ...  0.361619        0.607670         0.668692   \n",
      "1984-04-01         0.967858  ...  0.349963        0.714927         0.940134   \n",
      "1984-05-01         1.616333  ...  0.342990        0.662958         0.638484   \n",
      "\n",
      "            Bank loans  Earnings forecasts  Oil market  Commodities  \\\n",
      "date                                                                  \n",
      "1984-01-01    0.933329            0.399046    0.601900     0.969055   \n",
      "1984-02-01    0.871277            0.374899    0.366250     0.958566   \n",
      "1984-03-01    0.922590            0.379671    0.510741     0.902969   \n",
      "1984-04-01    1.043243            0.460857    0.382244     0.991594   \n",
      "1984-05-01    1.043655            0.364019    0.551286     0.976761   \n",
      "\n",
      "            Options/VIX  Private equity/hedge funds    Retail  \n",
      "date                                                           \n",
      "1984-01-01     0.527420                    0.211533  0.535695  \n",
      "1984-02-01     0.592115                    0.266090  0.405470  \n",
      "1984-03-01     0.576864                    0.224074  0.439319  \n",
      "1984-04-01     0.718155                    0.246094  0.516684  \n",
      "1984-05-01     0.457018                    0.276822  0.551830  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "df_attention = (\n",
    "    pd.read_csv(DATA_PATH)\n",
    "    .assign(date = lambda x: pd.to_datetime(x[\"date\"]))\n",
    "    .set_index(\"date\")\n",
    "    .iloc[:, SELECTED_COLS]\n",
    "    * 100\n",
    ")\n",
    "print(df_attention.shape)\n",
    "print(df_attention.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-campaign",
   "metadata": {},
   "source": [
    "## B. US GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inclusive-indicator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172, 1)\n",
      "               GDPC1\n",
      "DATE                \n",
      "1980-01-01       NaN\n",
      "1980-04-01 -7.990637\n",
      "1980-07-01 -0.474542\n",
      "1980-10-01  7.670986\n",
      "1981-01-01  8.071104\n"
     ]
    }
   ],
   "source": [
    "df_gdp = web.DataReader(\n",
    "    \"GDPC1\",\n",
    "    \"fred\",\n",
    "    pd.to_datetime(\"1980-01-01\"),\n",
    "    pd.to_datetime(\"2022-12-31\"),\n",
    ")\n",
    "df_gdp = (\n",
    "    df_gdp\n",
    "    .div(df_gdp.shift(1))\n",
    "    .pipe(lambda x: 100 * (x ** 4 - 1))\n",
    ")\n",
    "print(df_gdp.shape)\n",
    "print(df_gdp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-digest",
   "metadata": {},
   "source": [
    "## C. Chicago Fed National Activity Index (CFNAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "soviet-tribe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(516, 1)\n",
      "            CFNAI\n",
      "DATE             \n",
      "1980-01-01   0.13\n",
      "1980-02-01  -0.52\n",
      "1980-03-01  -1.14\n",
      "1980-04-01  -2.19\n",
      "1980-05-01  -2.30\n"
     ]
    }
   ],
   "source": [
    "df_cfnai = web.DataReader(\n",
    "    \"CFNAI\",\n",
    "    \"fred\",\n",
    "    pd.to_datetime(\"1980-01-01\"),\n",
    "    pd.to_datetime(\"2022-12-31\"),\n",
    ")\n",
    "print(df_cfnai.shape)\n",
    "print(df_cfnai.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-asset",
   "metadata": {},
   "source": [
    "## D. Non-Farm Payrolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "employed-ending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(516, 1)\n",
      "              PAYEMS\n",
      "DATE                \n",
      "1980-01-01       NaN\n",
      "1980-02-01  0.091368\n",
      "1980-03-01  0.122061\n",
      "1980-04-01 -0.159478\n",
      "1980-05-01 -0.473331\n"
     ]
    }
   ],
   "source": [
    "df_nfp = web.DataReader(\n",
    "    \"PAYEMS\",\n",
    "    \"fred\",\n",
    "    pd.to_datetime(\"1980-01-01\"),\n",
    "    pd.to_datetime(\"2022-12-31\"),\n",
    ")\n",
    "\n",
    "df_nfp = (\n",
    "    df_nfp\n",
    "    .pipe(np.log)\n",
    "    .diff(1)\n",
    "    * 100\n",
    ")\n",
    "print(df_nfp.shape)\n",
    "print(df_nfp.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-magic",
   "metadata": {},
   "source": [
    "## E. Aruoba-Diebold-Scotti Business Conditions Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sealed-default",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23056, 1)\n",
      "            ADS_index\n",
      "date                 \n",
      "1960-03-01  -0.579788\n",
      "1960-03-02  -0.626985\n",
      "1960-03-03  -0.671207\n",
      "1960-03-04  -0.712469\n",
      "1960-03-05  -0.750784\n"
     ]
    }
   ],
   "source": [
    "ADS_PATH = \"https://www.philadelphiafed.org/-/media/frbp/assets/surveys-and-data/ads/ads_index_most_current_vintage.xlsx\"\n",
    "\n",
    "df_ads = (\n",
    "    pd.read_excel(ADS_PATH, names = [\"date\", \"ADS_index\"])\n",
    "    .assign(date = lambda x: pd.to_datetime(x[\"date\"].str.replace(':', '-')))\n",
    "    .set_index(\"date\")\n",
    ")\n",
    "print(df_ads.shape)\n",
    "print(df_ads.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-disability",
   "metadata": {},
   "source": [
    "## F. Consolidating and transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "verified-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "dastart = \"1990-01-01\" # Beginning of training period\n",
    "daend_train = \"2002-01-01\" # End of training period\n",
    "\n",
    "# Concatenate the data, resample at monthly frequency using latest value, and standardize using EMAs.\n",
    "df_ = (\n",
    "    pd.concat([df_gdp, df_cfnai, df_nfp, df_ads, df_attention], axis=1)\n",
    "    .dropna(how=\"all\")\n",
    "    .resample('M').last()\n",
    "    .loc[dastart:]\n",
    "    .pipe(ts_standardize)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-renewal",
   "metadata": {},
   "source": [
    "# 2. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fifteen-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cov_lags = create_lags(df_.drop(\"GDPC1\", axis=1), l_max=9).ffill(limit=3) # Lags of covariates\n",
    "y_lags = create_lags(df_[[\"GDPC1\"]].resample('3M').last(), l_max=4) # Lags of outcome\n",
    "\n",
    "Xy_ = pd.concat([y_lags, X_cov_lags], axis=1).dropna().swaplevel(axis=1).sort_index(axis=1) # Putting everything together and dropping NAs when outcome is not here.\n",
    "y, X = Xy_[('GDPC1', 0)], Xy_.drop(('GDPC1', 0), axis=1) # Assign outcome and regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hybrid-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dict = {\n",
    "    g: np.where(X.columns.get_level_values(0) == g)[0].tolist() for g in X.columns.get_level_values(0).unique()\n",
    "}\n",
    "\n",
    "groups = list(group_dict.values())\n",
    "\n",
    "estimators = {\n",
    "    'SparseGroupLasso': Pipeline([\n",
    "        ('regression', SparseGroupLasso(groups=groups, gamma=0.8, alpha=1, verbose=False, max_iter=1_000))\n",
    "    ]),\n",
    "    'SVD with Ridge rotation': Pipeline([\n",
    "        ('svd', SVD(alpha=1e4)),\n",
    "        ('ols', LinearRegression())\n",
    "    ])\n",
    "\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "portable-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train period: index=1991-04-30 -> 1996-04-30\n",
      "  Test period:  index=1996-07-31 -> 2000-07-31\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Complex data not supported\n[0.15676661+0.j 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j\n 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j\n 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j\n 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j\n 0.15676661+0.j]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-12c272142a61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"R2 {name}: {est.score(X_[test_index], y_[test_index]):.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0mscore_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mscore_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/code/ml_econometrie/chapter11_now/src/stats_tools.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;31m#------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \"\"\"\n\u001b[1;32m    789\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m     )\n\u001b[1;32m    792\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;31m# result is that np.array(..) produces an array of complex dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;31m# and we need to catch and raise exception for such cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_no_complex_data\u001b[0;34m(array)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     ):\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Complex data not supported\n[0.15676661+0.j 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j\n 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j\n 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j\n 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j 0.15676661+0.j\n 0.15676661+0.j]\n"
     ]
    }
   ],
   "source": [
    "X_, y_ = X.values, y.values\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(X_)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train period: index={Xy_.index[train_index].min().date()} -> {Xy_.index[train_index].max().date()}\")\n",
    "    print(f\"  Test period:  index={Xy_.index[test_index].min().date()} -> {Xy_.index[test_index].max().date()}\")\n",
    "    \n",
    "    for name, est in estimators.items():\n",
    "        est.fit(X_[train_index], y_[train_index])\n",
    "        print(f\"R2 {name}: {est.score(X_[test_index], y_[test_index]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "generous-harassment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284, 284)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.named_steps['svd'].rotation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaae37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
